{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.027939,
          "end_time": "2020-11-18T19:14:47.649632",
          "exception": false,
          "start_time": "2020-11-18T19:14:47.621693",
          "status": "completed"
        },
        "tags": [],
        "id": "y4D_ChZOuw75"
      },
      "source": [
        "We mentioned that [restricted Boltzmann machines](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms) were a very early type of neural network and the field of deep learning has evolved considerably since then.\n",
        "\n",
        "What happens if we apply a more contemporary neural network to the problem?\n",
        "> Well, as we'll see, it's possible, but not without its challenges.\n",
        "\n",
        "People started using deeper neural networks for recommender systems in 2015,which seems pretty recent but it's a long time in the context of current AI research.\n",
        "\n",
        "A group from the Australian National University published a paper called AutoRec: [Autoencoders Meet Collaborative Filtering](http://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf) and they used the topology you see here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02549,
          "end_time": "2020-11-18T19:14:47.700756",
          "exception": false,
          "start_time": "2020-11-18T19:14:47.675266",
          "status": "completed"
        },
        "tags": [],
        "id": "4-4G0tJMuw77"
      },
      "source": [
        "![image.png](attachment:image.png)\n",
        "\n",
        "It looks a lot more familiar to the sorts of networks covered in [introduction to deep learning](https://www.kaggle.com/learn/intro-to-deep-learning). \n",
        "\n",
        "You have three layers:\n",
        "1. an input layer on the bottom that contains individual ratings.\n",
        "2. a hidden layer. \n",
        "3. an output layer that gives us our predictions.\n",
        "\n",
        "A matrix of weights between the layers is maintained across every instance of this network, as well as a bias node for both the hidden and output layers.\n",
        "\n",
        "In the [paper](http://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf), they trained the network once per item, feeding in ratings from each user for that item in the input layer.\n",
        "\n",
        "A sigmoid activation function was used on the output.\n",
        "\n",
        "All in all, it's a pretty straightforward approach, and they reported slightly better results compared to using an [RBM](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms).\n",
        "\n",
        "But the implementation is a bit different. [RBMs](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms) just had separate bias terms for each pass, while here we have a whole separate set of weights to work with too.\n",
        "This sort of architecture also has the benefit of being a lot easier to implement in modern frameworks such as TensorFlow or Keras. But there's still one wrinkle:\n",
        "> the sparsity of the data we are working with.\n",
        "\n",
        "In the [paper](http://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf) they briefly mentioned that \"We only consider the contribution of observed ratings.\" So they were careful to process each path through this neural network individually, only propagating information from ratings that actually exist in the training data, and ignoring the contribution from input nodes that correspond to missing data from user-item pairs that weren't rated at all.\n",
        "This is still a tough thing to do in TensorFlow. While TensorFlow does have sparse tensors, there's no simple way to restrict the chain of matrix multiplications and additions needed to implement a neural network to just the input nodes with actual data in them.\n",
        "\n",
        "Any implementation you'll find of this using TensorFlow or Keras just ignores that problem, and models missing ratings as zeroes. You can still get decent results with enough effort, but it's a very fundamental problem to applying deep learning to recommender systems.\n",
        "\n",
        "\n",
        "This architecture is referred to as an autoencoder. The act of building up the weights and biases between the input and hidden layer is referred to as encoding the input.\n",
        "\n",
        "We are encoding the patterns in the input as a set of weights into that hidden layer.\n",
        "\n",
        "Then as we reconstruct the output in the weights between the hidden and output layers we are decoding it.\n",
        "\n",
        "So the first set of weights is the encoding stage and the second set is the decoding stage.\n",
        "\n",
        "Conceptually, this isn't really any different from what we were doing with [RBMs](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms). In an RBM we encoded on the forward pass and decoded on the backward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.026074,
          "end_time": "2020-11-18T19:14:47.752145",
          "exception": false,
          "start_time": "2020-11-18T19:14:47.726071",
          "status": "completed"
        },
        "tags": [],
        "id": "-fq_mkYEuw77"
      },
      "source": [
        "That problem hasn't stopped people from trying, however. A couple of ideas that I've seen used a few times are using deeper neural networks with more hidden layers, and one hot encoding the user and item data together into a single input layer.\n",
        "\n",
        "That's what you see in this architecture printed out from Keras.\n",
        "![image.png](attachment:image.png)\n",
        "> Credit: https://nipunbatra.github.io/blog/2017/recommend-keras.html\n",
        "\n",
        "Items are embedded on the left side, and users on the right. Both are flattened, and a dropout layer applied to prevent overfitting, and then they are concatenated together before feeding it all into a deep neural network.\n",
        "\n",
        "But it still has the problem of not being able to distinguish between missing ratings and ratings of a value zero.\n",
        "\n",
        "Fundamentally, it models missing ratings as a signal that a user really, really hated a given item, and that's just not an accurate representation of reality.\n",
        "\n",
        "In the end, this particular network was unable to outperform matrix factorization on the [ML100K](https://www.kaggle.com/snehal1409/movielens) dataset. \n",
        "\n",
        "In part, it's due to the sparsity of the data, and it's also because 100,000 ratings just isn't anywhere near enough data to train a network as complex as this.\n",
        "\n",
        "They actually can work quite well once you have the right tools. It's not hard, however, to implement an autoencoder in TensorFlow that just treats missing ratings as zero values, so let's give it a shot and see for ourselves how well it does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.02546,
          "end_time": "2020-11-18T19:14:47.803287",
          "exception": false,
          "start_time": "2020-11-18T19:14:47.777827",
          "status": "completed"
        },
        "tags": [],
        "id": "VOyuYa3uuw78"
      },
      "source": [
        "# Auto Rec Module\n",
        "\n",
        "Like our [RBM Example](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms), our implementation of using and Auto Encoder for recommendations uses tensorflow and it's structured in much the same way.\n",
        "\n",
        "As a reminder, an Auto Encoder is just a 3 layer neural network with an input layer, a hidden layer, and an output layer.\n",
        "\n",
        "Learning the weights between the input and hidden layer is called encoding.\n",
        "\n",
        "And reconstructing predictions with the weights between the hidden layer and the output layer is called decoding.\n",
        "\n",
        "But fundamentally, it's just a neural network with one hidden layer.\n",
        "\n",
        "In the MakeGraph function things are fundamentally different from the [RBM example](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms).\n",
        "\n",
        "It's not too complicated. You can see we're setting up the weights for the encoding and decoding here randomly initialized. Those weights are learned and shared for every user we trained the Auto Encoder with. We also set up a set of biases for both layers.\n",
        "\n",
        "This is a little bit different from what the original Auto Rec [paper](http://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf) proposed, they only had a single bias node shared across the entire layer, while we learned biases for each node, which is more in line with modern practices.\n",
        "\n",
        "Next we set up the layers themselves. Our input layer receives ratings for each item for a given user.\n",
        "\n",
        "We then construct our hidden layer by multiplying our input layer with the encoding weights and adding in the encoding bias terms then applying a sigmoid activation function to them.\n",
        "\n",
        "Our output layer applies the learned decoder weights and biases to what's in the hidden layer and applies the sigmoid activation function to that final result as well.\n",
        "\n",
        "This makes up our actual predicted ratings for every item for a given user.\n",
        "\n",
        "To measure air, we need to compare those predicted and those are in our original input layer. So we just copy that for use in our loss function.\n",
        "\n",
        "We define our loss function of MSE between out predicted and actual ratings and use the RMS Optimizer to minimize that error.\n",
        "\n",
        "You might try using more modern optimization functions such as Adam if you want to tinker a bit.\n",
        "\n",
        "The Train function kicks it all off in pretty much the same way that we did before with [RBM](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms). Let's shift our attention to the AutoRecAlgorithm module."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPoVb1e2djRM",
        "outputId": "8b767fcf-04cc-4d87-cb03-2dac67d0f8ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULqnzpjNFraZ",
        "outputId": "d0d8ad71-8106-4e21-cac6-90421f898c1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:47.879580Z",
          "iopub.status.busy": "2020-11-18T19:14:47.878420Z",
          "iopub.status.idle": "2020-11-18T19:14:52.435333Z",
          "shell.execute_reply": "2020-11-18T19:14:52.433981Z"
        },
        "papermill": {
          "duration": 4.606262,
          "end_time": "2020-11-18T19:14:52.435547",
          "exception": false,
          "start_time": "2020-11-18T19:14:47.829285",
          "status": "completed"
        },
        "tags": [],
        "id": "uPRxa-zauw78"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "class AutoRec(object):\n",
        "\n",
        "    def __init__(self, visibleDimensions, epochs=800, hiddenDimensions=50, learningRate=0.1, batchSize=200):\n",
        "\n",
        "        self.visibleDimensions = visibleDimensions\n",
        "        self.epochs = epochs\n",
        "        self.hiddenDimensions = hiddenDimensions\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "        self.optimizer = tf.keras.optimizers.RMSprop(self.learningRate)\n",
        "\n",
        "\n",
        "    def Train(self, X):\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            for i in range(0, X.shape[0], self.batchSize):\n",
        "                epochX = X[i:i+self.batchSize]\n",
        "                self.run_optimization(epochX)\n",
        "\n",
        "\n",
        "            print(\"Trained epoch \", epoch)\n",
        "\n",
        "    def GetRecommendations(self, inputUser):\n",
        "\n",
        "        # Feed through a single user and return predictions from the output layer.\n",
        "        rec = self.neural_net(inputUser)\n",
        "\n",
        "        # It is being used as the return type is Eager Tensor.\n",
        "        return rec[0]\n",
        "\n",
        "\n",
        "    def neural_net(self, inputUser):\n",
        "\n",
        "        #tf.set_random_seed(0)\n",
        "\n",
        "        # Create varaibles for weights for the encoding (visible->hidden) and decoding (hidden->output) stages, randomly initialized\n",
        "        self.weights = {\n",
        "            'h1': tf.Variable(tf.random.normal([self.visibleDimensions, self.hiddenDimensions])),\n",
        "            'out': tf.Variable(tf.random.normal([self.hiddenDimensions, self.visibleDimensions]))\n",
        "            }\n",
        "\n",
        "        # Create biases\n",
        "        self.biases = {\n",
        "            'b1': tf.Variable(tf.random.normal([self.hiddenDimensions])),\n",
        "            'out': tf.Variable(tf.random.normal([self.visibleDimensions]))\n",
        "            }\n",
        "\n",
        "        # Create the input layer\n",
        "        self.inputLayer = inputUser\n",
        "\n",
        "        # hidden layer\n",
        "        hidden = tf.nn.sigmoid(tf.add(tf.matmul(self.inputLayer, self.weights['h1']), self.biases['b1']))\n",
        "\n",
        "        # output layer for our predictions.\n",
        "        self.outputLayer = tf.nn.sigmoid(tf.add(tf.matmul(hidden, self.weights['out']), self.biases['out']))\n",
        "\n",
        "        return self.outputLayer\n",
        "\n",
        "    def run_optimization(self, inputUser):\n",
        "        with tf.GradientTape() as g:\n",
        "            pred = self.neural_net(inputUser)\n",
        "            loss = tf.keras.losses.MSE(inputUser, pred)\n",
        "\n",
        "        trainable_variables = list(self.weights.values()) + list(self.biases.values())\n",
        "\n",
        "        gradients = g.gradient(loss, trainable_variables)\n",
        "\n",
        "        self.optimizer.apply_gradients(zip(gradients, trainable_variables))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.031693,
          "end_time": "2020-11-18T19:14:52.507723",
          "exception": false,
          "start_time": "2020-11-18T19:14:52.476030",
          "status": "completed"
        },
        "tags": [],
        "id": "7gNt1p-yuw79"
      },
      "source": [
        "# AutoRecAlgorithm Module\n",
        "\n",
        "There are a few things that have changed from the [RBM example](https://www.kaggle.com/alirezanematolahy/recommender-system-with-rbms) worth talking about.\n",
        "First of all, we're not modeling this as a classification problem where each individual rating value between zero and five stars are treated as a different input and output node.\n",
        "\n",
        "Instead we just normalize our input ratings into the range zero to one and restore them to their original ranges.\n",
        "\n",
        "So we don't have to deal with Soft Max and Expectation Values and all that in this example.\n",
        "\n",
        "As such our input matrix that passed into our Train function is just a 2D array of ratings between users and items.\n",
        "\n",
        "By the way this is different from what was in the [original Auto Rec paper](http://users.cecs.anu.edu.au/~akmenon/papers/autorec/autorec-paper.pdf).\n",
        "\n",
        "They flipped things so they trained the network on items instead of users. They would feed in all of the ratings by each user for a given item while we're feeding in the ratings for every item for each given user.\n",
        "\n",
        "Again, if you want to tinker you might try doing it the other way to see if you get better results."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-surprise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZXa1wuAvZ8m",
        "outputId": "20fc0771-0ae2-44d3-abff-3760df325424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:52.606225Z",
          "iopub.status.busy": "2020-11-18T19:14:52.605025Z",
          "iopub.status.idle": "2020-11-18T19:14:52.766889Z",
          "shell.execute_reply": "2020-11-18T19:14:52.768146Z"
        },
        "papermill": {
          "duration": 0.215366,
          "end_time": "2020-11-18T19:14:52.768357",
          "exception": false,
          "start_time": "2020-11-18T19:14:52.552991",
          "status": "completed"
        },
        "tags": [],
        "id": "Wu78T5Xnuw79"
      },
      "outputs": [],
      "source": [
        "from surprise import AlgoBase\n",
        "from surprise import PredictionImpossible\n",
        "import numpy as np\n",
        "\n",
        "class AutoRecAlgorithm(AlgoBase):\n",
        "\n",
        "    def __init__(self, epochs=100, hiddenDim=100, learningRate=0.01, batchSize=100, sim_options={}):\n",
        "        AlgoBase.__init__(self)\n",
        "        self.epochs = epochs\n",
        "        self.hiddenDim = hiddenDim\n",
        "        self.learningRate = learningRate\n",
        "        self.batchSize = batchSize\n",
        "\n",
        "    def fit(self, trainset):\n",
        "        AlgoBase.fit(self, trainset)\n",
        "\n",
        "        numUsers = trainset.n_users\n",
        "        numItems = trainset.n_items\n",
        "\n",
        "        trainingMatrix = np.zeros([numUsers, numItems], dtype=np.float32)\n",
        "\n",
        "        for (uid, iid, rating) in trainset.all_ratings():\n",
        "            trainingMatrix[int(uid), int(iid)] = rating / 5.0\n",
        "\n",
        "        # Create an RBM with (num items * rating values) visible nodes\n",
        "        autoRec = AutoRec(trainingMatrix.shape[1], hiddenDimensions=self.hiddenDim, learningRate=self.learningRate, batchSize=self.batchSize, epochs=self.epochs)\n",
        "        autoRec.Train(trainingMatrix)\n",
        "\n",
        "        self.predictedRatings = np.zeros([numUsers, numItems], dtype=np.float32)\n",
        "\n",
        "        for uiid in range(trainset.n_users):\n",
        "            if (uiid % 50 == 0):\n",
        "                print(\"Processing user \", uiid)\n",
        "            recs = autoRec.GetRecommendations([trainingMatrix[uiid]])\n",
        "\n",
        "            for itemID, rec in enumerate(recs):\n",
        "                self.predictedRatings[uiid, itemID] = rec * 5.0\n",
        "\n",
        "        return self\n",
        "\n",
        "\n",
        "    def estimate(self, u, i):\n",
        "\n",
        "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
        "            raise PredictionImpossible('User and/or item is unkown.')\n",
        "\n",
        "        rating = self.predictedRatings[u, i]\n",
        "\n",
        "        if (rating < 0.001):\n",
        "            raise PredictionImpossible('No valid prediction exists.')\n",
        "\n",
        "        return rating\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.041039,
          "end_time": "2020-11-18T19:14:52.852823",
          "exception": false,
          "start_time": "2020-11-18T19:14:52.811784",
          "status": "completed"
        },
        "tags": [],
        "id": "IF7uU6G3uw79"
      },
      "source": [
        "# Preparing data :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:52.949313Z",
          "iopub.status.busy": "2020-11-18T19:14:52.947982Z",
          "iopub.status.idle": "2020-11-18T19:14:53.007862Z",
          "shell.execute_reply": "2020-11-18T19:14:53.009163Z"
        },
        "papermill": {
          "duration": 0.114507,
          "end_time": "2020-11-18T19:14:53.009358",
          "exception": false,
          "start_time": "2020-11-18T19:14:52.894851",
          "status": "completed"
        },
        "tags": [],
        "id": "tAttf_8Vuw79"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import sys\n",
        "import re\n",
        "\n",
        "from surprise import Dataset\n",
        "from surprise import Reader\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "class MovieLens:\n",
        "\n",
        "    movieID_to_name = {}\n",
        "    name_to_movieID = {}\n",
        "    ratingsPath = '/content/drive/MyDrive/VAE_Updated_14th May/ml-20m/ratings_50k.csv'\n",
        "    moviesPath = '/content/drive/MyDrive/VAE_Updated_14th May/ml-20m/movies_28k.csv'\n",
        "    def loadMovieLensLatestSmall(self):\n",
        "\n",
        "        # Look for files relative to the directory we are running from\n",
        "        os.chdir(os.path.dirname(sys.argv[0]))\n",
        "\n",
        "        ratingsDataset = 0\n",
        "        self.movieID_to_name = {}\n",
        "        self.name_to_movieID = {}\n",
        "\n",
        "        reader = Reader(line_format='user item rating timestamp', sep=',', skip_lines=1)\n",
        "\n",
        "        ratingsDataset = Dataset.load_from_file(self.ratingsPath, reader=reader)\n",
        "\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "                movieReader = csv.reader(csvfile)\n",
        "                next(movieReader)  #Skip header line\n",
        "                for row in movieReader:\n",
        "                    movieID = int(row[0])\n",
        "                    movieName = row[1]\n",
        "                    self.movieID_to_name[movieID] = movieName\n",
        "                    self.name_to_movieID[movieName] = movieID\n",
        "\n",
        "        print(ratingsDataset)                                \n",
        "\n",
        "        return ratingsDataset\n",
        "\n",
        "    def getUserRatings(self, user):\n",
        "        userRatings = []\n",
        "        hitUser = False\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                userID = int(row[0])\n",
        "                if (user == userID):\n",
        "                    movieID = int(row[1])\n",
        "                    rating = float(row[2])\n",
        "                    userRatings.append((movieID, rating))\n",
        "                    hitUser = True\n",
        "                if (hitUser and (user != userID)):\n",
        "                    break\n",
        "\n",
        "        print(userRatings)\n",
        "\n",
        "        return userRatings\n",
        "\n",
        "    def getPopularityRanks(self):\n",
        "        ratings = defaultdict(int)\n",
        "        rankings = defaultdict(int)\n",
        "        with open(self.ratingsPath, newline='') as csvfile:\n",
        "            ratingReader = csv.reader(csvfile)\n",
        "            next(ratingReader)\n",
        "            for row in ratingReader:\n",
        "                movieID = int(row[1])\n",
        "                ratings[movieID] += 1\n",
        "        rank = 1\n",
        "        for movieID, ratingCount in sorted(ratings.items(), key=lambda x: x[1], reverse=True):\n",
        "            rankings[movieID] = rank\n",
        "            rank += 1\n",
        "        return rankings\n",
        "\n",
        "    def getGenres(self):\n",
        "        genres = defaultdict(list)\n",
        "        genreIDs = {}\n",
        "        maxGenreID = 0\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            movieReader = csv.reader(csvfile)\n",
        "            next(movieReader)  #Skip header line\n",
        "            for row in movieReader:\n",
        "                movieID = int(row[0])\n",
        "                genreList = row[2].split('|')\n",
        "                genreIDList = []\n",
        "                for genre in genreList:\n",
        "                    if genre in genreIDs:\n",
        "                        genreID = genreIDs[genre]\n",
        "                    else:\n",
        "                        genreID = maxGenreID\n",
        "                        genreIDs[genre] = genreID\n",
        "                        maxGenreID += 1\n",
        "                    genreIDList.append(genreID)\n",
        "                genres[movieID] = genreIDList\n",
        "        # Convert integer-encoded genre lists to bitfields that we can treat as vectors\n",
        "        for (movieID, genreIDList) in genres.items():\n",
        "            bitfield = [0] * maxGenreID\n",
        "            for genreID in genreIDList:\n",
        "                bitfield[genreID] = 1\n",
        "            genres[movieID] = bitfield\n",
        "\n",
        "        return genres\n",
        "\n",
        "    def getYears(self):\n",
        "        p = re.compile(r\"(?:\\((\\d{4})\\))?\\s*$\")\n",
        "        years = defaultdict(int)\n",
        "        with open(self.moviesPath, newline='', encoding='ISO-8859-1') as csvfile:\n",
        "            movieReader = csv.reader(csvfile)\n",
        "            next(movieReader)\n",
        "            for row in movieReader:\n",
        "                movieID = int(row[0])\n",
        "                title = row[1]\n",
        "                m = p.search(title)\n",
        "                year = m.group(1)\n",
        "                if year:\n",
        "                    years[movieID] = int(year)\n",
        "        return years\n",
        "\n",
        "    def getMiseEnScene(self):\n",
        "        mes = defaultdict(list)\n",
        "        with open(\"LLVisualFeatures13K_Log.csv\", newline='') as csvfile:\n",
        "            mesReader = csv.reader(csvfile)\n",
        "            next(mesReader)\n",
        "            for row in mesReader:\n",
        "                movieID = int(row[0])\n",
        "                avgShotLength = float(row[1])\n",
        "                meanColorVariance = float(row[2])\n",
        "                stddevColorVariance = float(row[3])\n",
        "                meanMotion = float(row[4])\n",
        "                stddevMotion = float(row[5])\n",
        "                meanLightingKey = float(row[6])\n",
        "                numShots = float(row[7])\n",
        "                mes[movieID] = [avgShotLength, meanColorVariance, stddevColorVariance,\n",
        "                   meanMotion, stddevMotion, meanLightingKey, numShots]\n",
        "        return mes\n",
        "\n",
        "    def getMovieName(self, movieID):\n",
        "        if movieID in self.movieID_to_name:\n",
        "            return self.movieID_to_name[movieID]\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    def getMovieID(self, movieName):\n",
        "        if movieName in self.name_to_movieID:\n",
        "            return self.name_to_movieID[movieName]\n",
        "        else:\n",
        "            return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.04408,
          "end_time": "2020-11-18T19:14:53.131817",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.087737",
          "status": "completed"
        },
        "tags": [],
        "id": "KxFRT_JNuw7-"
      },
      "source": [
        "# Evaluation Data :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:53.227259Z",
          "iopub.status.busy": "2020-11-18T19:14:53.226311Z",
          "iopub.status.idle": "2020-11-18T19:14:53.241801Z",
          "shell.execute_reply": "2020-11-18T19:14:53.242950Z"
        },
        "papermill": {
          "duration": 0.069128,
          "end_time": "2020-11-18T19:14:53.243128",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.174000",
          "status": "completed"
        },
        "tags": [],
        "id": "RDhcfpYTuw7-"
      },
      "outputs": [],
      "source": [
        "from surprise.model_selection import train_test_split\n",
        "from surprise.model_selection import LeaveOneOut\n",
        "from surprise import KNNBaseline\n",
        "\n",
        "class EvaluationData:\n",
        "\n",
        "    def __init__(self, data, popularityRankings):\n",
        "\n",
        "        self.rankings = popularityRankings\n",
        "\n",
        "        #Build a full training set for evaluating overall properties\n",
        "        self.fullTrainSet = data.build_full_trainset()\n",
        "        self.fullAntiTestSet = self.fullTrainSet.build_anti_testset()\n",
        "\n",
        "        #Build a 75/25 train/test split for measuring accuracy\n",
        "        self.trainSet, self.testSet = train_test_split(data, test_size=.25, random_state=1)\n",
        "\n",
        "        #Build a \"leave one out\" train/test split for evaluating top-N recommenders\n",
        "        #And build an anti-test-set for building predictions\n",
        "        LOOCV = LeaveOneOut(n_splits=1, random_state=1)\n",
        "        for train, test in LOOCV.split(data):\n",
        "            self.LOOCVTrain = train\n",
        "            self.LOOCVTest = test\n",
        "\n",
        "        self.LOOCVAntiTestSet = self.LOOCVTrain.build_anti_testset()\n",
        "\n",
        "        #Compute similarty matrix between items so we can measure diversity\n",
        "        sim_options = {'name': 'cosine', 'user_based': False}\n",
        "        self.simsAlgo = KNNBaseline(sim_options=sim_options)\n",
        "        self.simsAlgo.fit(self.fullTrainSet)\n",
        "\n",
        "    def GetFullTrainSet(self):\n",
        "        return self.fullTrainSet\n",
        "\n",
        "    def GetFullAntiTestSet(self):\n",
        "        return self.fullAntiTestSet\n",
        "\n",
        "    def GetAntiTestSetForUser(self, testSubject):\n",
        "        trainset = self.fullTrainSet\n",
        "        fill = trainset.global_mean\n",
        "        anti_testset = []\n",
        "        u = trainset.to_inner_uid(str(testSubject))\n",
        "        user_items = set([j for (j, _) in trainset.ur[u]])\n",
        "        anti_testset += [(trainset.to_raw_uid(u), trainset.to_raw_iid(i), fill) for\n",
        "                                 i in trainset.all_items() if\n",
        "                                 i not in user_items]\n",
        "        return anti_testset\n",
        "\n",
        "    def GetTrainSet(self):\n",
        "        return self.trainSet\n",
        "\n",
        "    def GetTestSet(self):\n",
        "        return self.testSet\n",
        "\n",
        "    def GetLOOCVTrainSet(self):\n",
        "        return self.LOOCVTrain\n",
        "\n",
        "    def GetLOOCVTestSet(self):\n",
        "        return self.LOOCVTest\n",
        "\n",
        "    def GetLOOCVAntiTestSet(self):\n",
        "        return self.LOOCVAntiTestSet\n",
        "\n",
        "    def GetSimilarities(self):\n",
        "        return self.simsAlgo\n",
        "\n",
        "    def GetPopularityRankings(self):\n",
        "        return self.rankings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.036219,
          "end_time": "2020-11-18T19:14:53.317552",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.281333",
          "status": "completed"
        },
        "tags": [],
        "id": "c0DO7HlRuw7-"
      },
      "source": [
        "# Evaluator Module :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:53.416812Z",
          "iopub.status.busy": "2020-11-18T19:14:53.414627Z",
          "iopub.status.idle": "2020-11-18T19:14:53.417582Z",
          "shell.execute_reply": "2020-11-18T19:14:53.418101Z"
        },
        "papermill": {
          "duration": 0.062402,
          "end_time": "2020-11-18T19:14:53.418235",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.355833",
          "status": "completed"
        },
        "tags": [],
        "id": "zAF8lhHQuw7-"
      },
      "outputs": [],
      "source": [
        "class Evaluator:\n",
        "\n",
        "    algorithms = []\n",
        "\n",
        "    def __init__(self, dataset, rankings):\n",
        "        ed = EvaluationData(dataset, rankings)\n",
        "        self.dataset = ed\n",
        "\n",
        "    def AddAlgorithm(self, algorithm, name):\n",
        "        alg = EvaluatedAlgorithm(algorithm, name)\n",
        "        self.algorithms.append(alg)\n",
        "\n",
        "    def Evaluate(self, doTopN):\n",
        "        results = {}\n",
        "        for algorithm in self.algorithms:\n",
        "            print(\"Evaluating \", algorithm.GetName(), \"...\")\n",
        "            results[algorithm.GetName()] = algorithm.Evaluate(self.dataset, doTopN)\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\")\n",
        "\n",
        "        if (doTopN):\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"AutoRec Map output: \", metrics[\"Map\"])\n",
        "            print(\"{:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
        "                    \"Algorithm\", \"RMSE\", \"MAE\", \"HR\", \"cHR\", \"ARHR\", \"Coverage\", \"Diversity\", \"Novelty\"))\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
        "                        name, metrics[\"RMSE\"], metrics[\"MAE\"], metrics[\"HR\"], metrics[\"cHR\"], metrics[\"ARHR\"],\n",
        "                                      metrics[\"Coverage\"], metrics[\"Diversity\"], metrics[\"Novelty\"]))\n",
        "        else:\n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"Random Map output: \", metrics[\"Map\"])\n",
        "            print(\"{:<10} {:<10} {:<10}\".format(\"Algorithm\", \"RMSE\", \"MAE\"))\n",
        "            \n",
        "            for (name, metrics) in results.items():\n",
        "                print(\"{:<10} {:<10.4f} {:<10.4f}\".format(name, metrics[\"RMSE\"], metrics[\"MAE\"]))\n",
        "\n",
        "        print(\"\\nLegend:\\n\")\n",
        "        print(\"RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\")\n",
        "        print(\"MAE:       Mean Absolute Error. Lower values mean better accuracy.\")\n",
        "        print(\"Map:       Mean Average Precision.\")\n",
        "        if (doTopN):\n",
        "            print(\"HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\")\n",
        "            print(\"cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\")\n",
        "            print(\"ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\" )\n",
        "            print(\"Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\")\n",
        "            print(\"Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\")\n",
        "            print(\"           for a given user. Higher means more diverse.\")\n",
        "            print(\"Novelty:   Average popularity rank of recommended items. Higher means more novel.\")\n",
        "\n",
        "    def SampleTopNRecs(self, ml, testSubject=10, k=10):\n",
        "\n",
        "        for algo in self.algorithms:\n",
        "            print(\"\\nUsing recommender \", algo.GetName())\n",
        "\n",
        "            print(\"\\nBuilding recommendation model...\")\n",
        "            trainSet = self.dataset.GetFullTrainSet()\n",
        "            algo.GetAlgorithm().fit(trainSet)\n",
        "\n",
        "            print(\"Computing recommendations...\")\n",
        "            testSet = self.dataset.GetAntiTestSetForUser(testSubject)\n",
        "\n",
        "            predictions = algo.GetAlgorithm().test(testSet)\n",
        "\n",
        "            recommendations = []\n",
        "\n",
        "            print (\"\\nWe recommend:\")\n",
        "            for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "                intMovieID = int(movieID)\n",
        "                recommendations.append((intMovieID, estimatedRating))\n",
        "\n",
        "            recommendations.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for ratings in recommendations[:10]:\n",
        "                print(ml.getMovieName(ratings[0]), ratings[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.035341,
          "end_time": "2020-11-18T19:14:53.490218",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.454877",
          "status": "completed"
        },
        "tags": [],
        "id": "0pZ00ptBuw7-"
      },
      "source": [
        "# Recommender Metrics :"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "from surprise import Dataset\n",
        "from surprise import SVD\n",
        "from surprise.model_selection import KFold\n",
        "\n",
        "\n",
        "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
        "    \"\"\"Return precision and recall at k metrics for each user\"\"\"\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        # When n_rec_k is 0, Precision is undefined. We here set it to 0.\n",
        "\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 0\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        # When n_rel is 0, Recall is undefined. We here set it to 0.\n",
        "\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 0\n",
        "\n",
        "    return precisions\n",
        "    "
      ],
      "metadata": {
        "id": "8Ba_32rK94uZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:53.581465Z",
          "iopub.status.busy": "2020-11-18T19:14:53.580560Z",
          "iopub.status.idle": "2020-11-18T19:14:53.624851Z",
          "shell.execute_reply": "2020-11-18T19:14:53.626103Z"
        },
        "papermill": {
          "duration": 0.098065,
          "end_time": "2020-11-18T19:14:53.626296",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.528231",
          "status": "completed"
        },
        "tags": [],
        "id": "dznrZTcuuw7_"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "from surprise import accuracy\n",
        "from collections import defaultdict\n",
        "\n",
        "class RecommenderMetrics:\n",
        "\n",
        "    def MAE(predictions):\n",
        "        return accuracy.mae(predictions, verbose=False)\n",
        "\n",
        "    def RMSE(predictions):\n",
        "        return accuracy.rmse(predictions, verbose=False)\n",
        "\n",
        "\n",
        "    def Map(predictions):\n",
        "        return precision_recall_at_k(predictions, k=5, threshold=4)\n",
        "\n",
        "\n",
        "    def GetTopN(predictions, n=10, minimumRating=0.0):\n",
        "        topN = defaultdict(list)\n",
        "\n",
        "\n",
        "        for userID, movieID, actualRating, estimatedRating, _ in predictions:\n",
        "            if (estimatedRating >= minimumRating):\n",
        "                topN[int(userID)].append((int(movieID), estimatedRating))\n",
        "\n",
        "        for userID, ratings in topN.items():\n",
        "            ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "            topN[int(userID)] = ratings[:n]\n",
        "\n",
        "        return topN\n",
        "\n",
        "    def HitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for leftOut in leftOutPredictions:\n",
        "            userID = leftOut[0]\n",
        "            leftOutMovieID = leftOut[1]\n",
        "            # Is it in the predicted top 10 for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == int(movieID)):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits += 1\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "    def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):\n",
        "        hits = 0\n",
        "        total = 0\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Only look at ability to recommend things the users actually liked...\n",
        "            if (actualRating >= ratingCutoff):\n",
        "                # Is it in the predicted top 10 for this user?\n",
        "                hit = False\n",
        "                for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                    if (int(leftOutMovieID) == movieID):\n",
        "                        hit = True\n",
        "                        break\n",
        "                if (hit) :\n",
        "                    hits += 1\n",
        "\n",
        "                total += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        return hits/total\n",
        "\n",
        "\n",
        "# Commenting this out\n",
        "\n",
        "    def RatingHitRate(topNPredicted, leftOutPredictions):\n",
        "        hits = defaultdict(float)\n",
        "        total = defaultdict(float)\n",
        "\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit) :\n",
        "                hits[actualRating] += 1\n",
        "\n",
        "            total[actualRating] += 1\n",
        "\n",
        "        # Compute overall precision\n",
        "        for rating in sorted(hits.keys()):\n",
        "            print (rating, hits[rating] / total[rating])\n",
        "\n",
        "    def AverageReciprocalHitRank(topNPredicted, leftOutPredictions):\n",
        "        summation = 0\n",
        "        total = 0\n",
        "        # For each left-out rating\n",
        "        for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:\n",
        "            # Is it in the predicted top N for this user?\n",
        "            hitRank = 0\n",
        "            rank = 0\n",
        "            for movieID, predictedRating in topNPredicted[int(userID)]:\n",
        "                rank = rank + 1\n",
        "                if (int(leftOutMovieID) == movieID):\n",
        "                    hitRank = rank\n",
        "                    break\n",
        "            if (hitRank > 0) :\n",
        "                summation += 1.0 / hitRank\n",
        "\n",
        "            total += 1\n",
        "\n",
        "        return summation / total\n",
        "\n",
        "    # What percentage of users have at least one \"good\" recommendation\n",
        "    def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):\n",
        "        hits = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            hit = False\n",
        "            for movieID, predictedRating in topNPredicted[userID]:\n",
        "                if (predictedRating >= ratingThreshold):\n",
        "                    hit = True\n",
        "                    break\n",
        "            if (hit):\n",
        "                hits += 1\n",
        "\n",
        "        return hits / numUsers\n",
        "\n",
        "    def Diversity(topNPredicted, simsAlgo):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        simsMatrix = simsAlgo.compute_similarities()\n",
        "        for userID in topNPredicted.keys():\n",
        "            pairs = itertools.combinations(topNPredicted[userID], 2)\n",
        "            for pair in pairs:\n",
        "                movie1 = pair[0][0]\n",
        "                movie2 = pair[1][0]\n",
        "                innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1))\n",
        "                innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))\n",
        "                similarity = simsMatrix[innerID1][innerID2]\n",
        "                total += similarity\n",
        "                n += 1\n",
        "\n",
        "        if (n > 0):\n",
        "            S = total / n\n",
        "            return (1-S)\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def Novelty(topNPredicted, rankings):\n",
        "        n = 0\n",
        "        total = 0\n",
        "        for userID in topNPredicted.keys():\n",
        "            for rating in topNPredicted[userID]:\n",
        "                movieID = rating[0]\n",
        "                rank = rankings[movieID]\n",
        "                total += rank\n",
        "                n += 1\n",
        "        return total / n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.044478,
          "end_time": "2020-11-18T19:14:53.711502",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.667024",
          "status": "completed"
        },
        "tags": [],
        "id": "sjBNFhF9uw7_"
      },
      "source": [
        "# Evaluated Algorithm :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:53.810522Z",
          "iopub.status.busy": "2020-11-18T19:14:53.809519Z",
          "iopub.status.idle": "2020-11-18T19:14:53.813991Z",
          "shell.execute_reply": "2020-11-18T19:14:53.813432Z"
        },
        "papermill": {
          "duration": 0.05871,
          "end_time": "2020-11-18T19:14:53.814121",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.755411",
          "status": "completed"
        },
        "tags": [],
        "id": "Zp3ICzIuuw7_"
      },
      "outputs": [],
      "source": [
        "class EvaluatedAlgorithm:\n",
        "\n",
        "    def __init__(self, algorithm, name):\n",
        "        self.algorithm = algorithm\n",
        "        self.name = name\n",
        "\n",
        "    def Evaluate(self, evaluationData, doTopN, n=10, verbose=True):\n",
        "        metrics = {}\n",
        "        # Compute accuracy\n",
        "        if (verbose):\n",
        "            print(\"Evaluating accuracy...\")\n",
        "        self.algorithm.fit(evaluationData.GetTrainSet())\n",
        "        predictions = self.algorithm.test(evaluationData.GetTestSet())\n",
        "        metrics[\"RMSE\"] = RecommenderMetrics.RMSE(predictions)\n",
        "        metrics[\"MAE\"] = RecommenderMetrics.MAE(predictions)\n",
        "\n",
        "        metrics[\"Map\"] = RecommenderMetrics.Map(predictions)\n",
        "\n",
        "        if (doTopN):\n",
        "            # Evaluate top-10 with Leave One Out testing\n",
        "            if (verbose):\n",
        "                print(\"Evaluating top-N with leave-one-out...\")\n",
        "            self.algorithm.fit(evaluationData.GetLOOCVTrainSet())\n",
        "            leftOutPredictions = self.algorithm.test(evaluationData.GetLOOCVTestSet())\n",
        "            # Build predictions for all ratings not in the training set\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetLOOCVAntiTestSet())\n",
        "            # Compute top 10 recs for each user\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Computing hit-rate and rank metrics...\")\n",
        "            # See how often we recommended a movie the user actually rated\n",
        "            metrics[\"HR\"] = RecommenderMetrics.HitRate(topNPredicted, leftOutPredictions)\n",
        "            # See how often we recommended a movie the user actually liked\n",
        "            metrics[\"cHR\"] = RecommenderMetrics.CumulativeHitRate(topNPredicted, leftOutPredictions)\n",
        "            # Compute ARHR\n",
        "            metrics[\"ARHR\"] = RecommenderMetrics.AverageReciprocalHitRank(topNPredicted, leftOutPredictions)\n",
        "\n",
        "            #Evaluate properties of recommendations on full training set\n",
        "            if (verbose):\n",
        "                print(\"Computing recommendations with full data set...\")\n",
        "            self.algorithm.fit(evaluationData.GetFullTrainSet())\n",
        "            allPredictions = self.algorithm.test(evaluationData.GetFullAntiTestSet())\n",
        "            topNPredicted = RecommenderMetrics.GetTopN(allPredictions, n)\n",
        "            if (verbose):\n",
        "                print(\"Analyzing coverage, diversity, and novelty...\")\n",
        "            # Print user coverage with a minimum predicted rating of 4.0:\n",
        "            metrics[\"Coverage\"] = RecommenderMetrics.UserCoverage(  topNPredicted,\n",
        "                                                                   evaluationData.GetFullTrainSet().n_users,\n",
        "                                                                   ratingThreshold=4.0)\n",
        "            # Measure diversity of recommendations:\n",
        "            metrics[\"Diversity\"] = RecommenderMetrics.Diversity(topNPredicted, evaluationData.GetSimilarities())\n",
        "\n",
        "            # Measure novelty (average popularity rank of recommendations):\n",
        "            metrics[\"Novelty\"] = RecommenderMetrics.Novelty(topNPredicted,\n",
        "                                                            evaluationData.GetPopularityRankings())\n",
        "\n",
        "        if (verbose):\n",
        "            print(\"Analysis complete.\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def GetName(self):\n",
        "        return self.name\n",
        "\n",
        "    def GetAlgorithm(self):\n",
        "        return self.algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "papermill": {
          "duration": 0.031523,
          "end_time": "2020-11-18T19:14:53.871140",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.839617",
          "status": "completed"
        },
        "tags": [],
        "id": "IOZwZ1a_uw7_"
      },
      "source": [
        "# AutoRec Driver Module\n",
        "\n",
        "There's not much to talk about in what it does. It just uses our framework to compare Auto Rec to random recommendations as we've done before.\n",
        "\n",
        "Run the next cell and it will take several minutes for it to do all of its work as we've set this up to do all of the top end metrics as well as generate some sample top-N recommendations so we can get a more comprehensive picture of how its doing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-11-18T19:14:53.943511Z",
          "iopub.status.busy": "2020-11-18T19:14:53.942575Z",
          "iopub.status.idle": "2020-11-18T21:51:29.533090Z",
          "shell.execute_reply": "2020-11-18T21:51:29.532357Z"
        },
        "papermill": {
          "duration": 9395.631726,
          "end_time": "2020-11-18T21:51:29.533222",
          "exception": false,
          "start_time": "2020-11-18T19:14:53.901496",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsrhF8EEuw7_",
        "outputId": "47fc47ba-b10b-41ac-ec83-7c9e2ea8324a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading movie ratings...\n",
            "<surprise.dataset.DatasetAutoFolds object at 0x7f1db26034d0>\n",
            "\n",
            "Computing movie popularity ranks so we can measure novelty later...\n"
          ]
        }
      ],
      "source": [
        "from surprise import NormalPredictor\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def LoadMovieLensData():\n",
        "    ml = MovieLens()\n",
        "    print(\"Loading movie ratings...\")\n",
        "    data = ml.loadMovieLensLatestSmall()\n",
        "    print(\"\\nComputing movie popularity ranks so we can measure novelty later...\")\n",
        "    rankings = ml.getPopularityRanks()\n",
        "    return (ml, data, rankings)\n",
        "\n",
        "np.random.seed(0)\n",
        "random.seed(0)\n",
        "\n",
        "# Load up common data set for the recommender algorithms\n",
        "(ml, evaluationData, rankings) = LoadMovieLensData()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Construct an Evaluator to, you know, evaluate them\n",
        "evaluator = Evaluator(evaluationData, rankings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3enlAm7sfApq",
        "outputId": "c3da350a-3858-468c-8c79-e1bff773a9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimating biases using als...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Autoencoder\n",
        "AutoRecSys = AutoRecAlgorithm()\n"
      ],
      "metadata": {
        "id": "k7LF86_Nv83s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.AddAlgorithm(AutoRecSys, \"AutoRec\")\n"
      ],
      "metadata": {
        "id": "1IlD0SH1fLoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Just make random recommendations\n",
        "Random = NormalPredictor()\n"
      ],
      "metadata": {
        "id": "WOOYu95AfNL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluator.AddAlgorithm(Random, \"Random\")\n"
      ],
      "metadata": {
        "id": "odKGMeNEfO11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Fight!\n",
        "evaluator.Evaluate(True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YH5Pl3EvfQWD",
        "outputId": "ed01f142-9669-4162-d60e-46ba398b4f0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating  AutoRec ...\n",
            "Evaluating accuracy...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Evaluating top-N with leave-one-out...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "Evaluating  Random ...\n",
            "Evaluating accuracy...\n",
            "Evaluating top-N with leave-one-out...\n",
            "Computing hit-rate and rank metrics...\n",
            "Computing recommendations with full data set...\n",
            "Analyzing coverage, diversity, and novelty...\n",
            "Computing the cosine similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Analysis complete.\n",
            "\n",
            "\n",
            "AutoRec Map output:  {'54': 0.0, '61': 0.6, '232': 0.8, '164': 1.0, '366': 0.2, '295': 0.4, '278': 0.0, '88': 0.2, '43': 0.4, '285': 0.4, '156': 1.0, '247': 0.6, '119': 0.4, '69': 0.2, '98': 0.8, '309': 0.2, '31': 0.6, '204': 0.8, '42': 0.6, '190': 0.75, '215': 0.6, '104': 0.2, '236': 0.6, '99': 1.0, '367': 0.2, '192': 0.6, '96': 0.4, '208': 0.6, '347': 0.4, '136': 0.8, '160': 0.2, '306': 0.6, '336': 0.6, '209': 1.0, '188': 0.4, '147': 0.4, '132': 0.6, '56': 0.4, '116': 0.4, '176': 0, '296': 0.6, '58': 1.0, '53': 1.0, '298': 0.6, '165': 0.4, '359': 0.6, '40': 0.6, '22': 0.4, '135': 0.4, '46': 0.2, '91': 0.0, '30': 0.0, '33': 0.8, '73': 0.8, '348': 0.8, '130': 0.4, '72': 0.5, '237': 0.8, '32': 0.8, '218': 0.8, '82': 0.8, '318': 0.4, '319': 0.0, '344': 0.6, '89': 0.2, '205': 0.4, '133': 0.8, '141': 0.4, '27': 0.6, '332': 0.6, '241': 0.8, '11': 0.4, '245': 1.0, '131': 0.0, '108': 0.2, '114': 0.8, '258': 0.2, '167': 0.6, '370': 1.0, '24': 0.0, '263': 0.6, '124': 0.6, '337': 1.0, '361': 0.8, '41': 0.8, '240': 0.0, '238': 0.6, '21': 0.8, '10': 0.75, '174': 0, '137': 0.4, '115': 0.0, '186': 0.2, '346': 0.8, '312': 0.6, '45': 0.0, '279': 0.6, '212': 0.2, '314': 0.4, '140': 1.0, '251': 0.4, '3': 0.8, '83': 0.4, '14': 0.2, '101': 0.4, '145': 0.4, '134': 0.4, '181': 0.4, '271': 0.6, '152': 0.4, '18': 0.6, '224': 0.5, '357': 0.8, '97': 0.6, '202': 0.2, '142': 0.4, '311': 0.2, '150': 0.3333333333333333, '220': 0.4, '38': 0.6, '360': 1.0, '194': 1.0, '265': 0.0, '248': 0.8, '126': 0.4, '235': 0.4, '85': 0.8, '52': 0.75, '267': 1.0, '158': 0.4, '1': 0.4, '244': 0.8, '184': 0.6, '144': 1.0, '283': 0.4, '313': 0.4, '86': 0.6, '320': 0.6, '246': 0.2, '16': 0.8, '349': 0.4, '282': 0.5, '317': 0.4, '266': 1.0, '294': 0.2, '13': 0.6666666666666666, '196': 0.6, '154': 0.8, '9': 0.5, '293': 0.5, '195': 0.0, '100': 0.3333333333333333, '109': 0.2, '234': 0.4, '269': 0.4, '316': 0.2, '206': 0.4, '60': 0.6, '325': 0.6, '198': 0.6, '305': 1.0, '127': 0.6, '302': 1.0, '343': 0.0, '227': 0.3333333333333333, '65': 0.6, '34': 0.6, '354': 0.8, '201': 1.0, '168': 1.0, '105': 0, '128': 0.6, '330': 0.0, '277': 0.6, '103': 0.2, '189': 1.0, '129': 0.2, '328': 0.0, '292': 0.3333333333333333, '149': 0.0, '243': 0.5, '299': 0.0, '254': 0.4, '157': 0.4, '68': 0.5, '175': 0.4, '280': 1.0, '57': 0.75, '79': 0, '253': 1.0, '19': 1.0, '50': 0.6, '340': 0.4, '229': 0.4, '75': 0.5, '169': 1.0, '290': 0.6, '153': 1.0, '310': 1.0, '334': 0.2, '315': 1.0, '44': 1.0, '7': 0.4, '121': 0.6, '67': 0.5, '239': 0.6, '273': 0.4, '231': 1.0, '262': 1.0, '76': 1.0, '51': 0.4, '170': 0.8, '148': 0.6, '139': 0.0, '286': 1.0, '112': 0.4, '335': 1.0, '284': 0.2, '182': 0.6, '48': 0.4, '355': 1.0, '183': 0.4, '29': 0.8, '77': 0.4, '339': 1.0, '264': 0.2, '270': 0.8, '230': 0.8, '338': 0.6666666666666666, '368': 0.6, '35': 0.6, '172': 0.0, '173': 1.0, '256': 0.8, '327': 0.5, '304': 0.6, '90': 0.4, '118': 0.25, '185': 0.3333333333333333, '84': 0.6666666666666666, '62': 1.0, '64': 1.0, '20': 1.0, '23': 0.6, '5': 0.8, '213': 0.8, '6': 1.0, '369': 0.4, '70': 0.4, '331': 0.2, '199': 0.25, '303': 0.8, '326': 0, '55': 1.0, '250': 0.6666666666666666, '352': 0.0, '223': 0.6, '187': 0.0, '289': 0.4, '26': 0.0, '178': 1.0, '113': 0.6666666666666666, '233': 0.4, '323': 0.0, '228': 0.6666666666666666, '351': 0.6, '291': 1.0, '74': 0.8, '210': 1.0, '71': 1.0, '106': 0.25, '120': 0.2, '358': 0.6, '197': 1.0, '211': 0.8, '219': 0.4, '163': 0.6, '342': 0.8, '180': 0.0, '161': 0.0, '93': 1.0, '249': 0.5, '365': 0.4, '92': 0.4, '307': 0.6666666666666666, '151': 0.2, '125': 0.5, '25': 0.6, '122': 0.75, '123': 0.0, '59': 0.5, '102': 0.6, '225': 0.4, '222': 0.75, '162': 0.6, '200': 0.6, '110': 0.4, '8': 0, '117': 0.8, '329': 0.0, '47': 0.0, '364': 0.5, '252': 0.4, '301': 0.0, '321': 0.6, '276': 1.0, '37': 0.6666666666666666, '203': 1.0, '2': 0.5, '322': 0.3333333333333333, '66': 0.6, '138': 0.0, '255': 1.0, '324': 0.3333333333333333, '49': 0.6666666666666666, '217': 0.3333333333333333, '193': 0.0, '39': 0.0, '111': 0.4, '107': 0.6, '268': 0.6666666666666666, '207': 0.8, '155': 0.6, '171': 1.0, '272': 0.6, '281': 1.0, '78': 0.8, '274': 0.0, '300': 1.0, '363': 0.5, '259': 0.4, '275': 1.0, '28': 0.5, '143': 1.0, '166': 0.5, '191': 0.5, '333': 1.0, '12': 0.5, '288': 0.6666666666666666, '350': 0.0, '63': 1.0, '242': 0.0, '214': 0.6666666666666666, '345': 0.3333333333333333, '159': 1.0, '257': 0, '4': 1.0, '94': 0.2, '80': 1.0, '81': 1.0, '221': 1.0, '308': 0.5, '261': 0.5, '260': 0.2, '95': 0.0, '146': 0.6666666666666666, '297': 0.0, '15': 0.0, '353': 0, '36': 0.0, '177': 1.0, '87': 0.0, '179': 0, '216': 1.0, '341': 0.6666666666666666, '226': 1.0, '356': 1.0, '17': 1.0, '287': 1.0, '362': 0.0}\n",
            "AutoRec Map output:  {'54': 0.4, '61': 0.8, '232': 0.8, '164': 0.8, '366': 0.0, '295': 0.0, '278': 0.2, '88': 0.2, '43': 0.0, '285': 0.6, '156': 0.4, '247': 0.4, '119': 1.0, '69': 0.6, '98': 1.0, '309': 0.8, '31': 0.0, '204': 1.0, '42': 0.6, '190': 0.8, '215': 0.4, '104': 0.4, '236': 1.0, '99': 0.75, '367': 0.8, '192': 0.8, '96': 0.6, '208': 1.0, '347': 0.8, '136': 0.8, '160': 0.2, '306': 0.4, '336': 0.4, '209': 0.4, '188': 0.0, '147': 0.6, '132': 0.4, '56': 0.0, '116': 0.2, '176': 0, '296': 0.6666666666666666, '58': 1.0, '53': 0.8, '298': 0.2, '165': 0.0, '359': 0.8, '40': 0.5, '22': 0.6, '135': 0.4, '46': 0.4, '91': 0.4, '30': 0.0, '33': 0.4, '73': 1.0, '348': 0.8, '130': 0.6, '72': 0.6, '237': 1.0, '32': 0.0, '218': 0.2, '82': 0.8, '318': 0.2, '319': 0.0, '344': 1.0, '89': 0.2, '205': 0.4, '133': 0.4, '141': 0.3333333333333333, '27': 0.8, '332': 0.6, '241': 0.4, '11': 0.8, '245': 0.6, '131': 0.0, '108': 0.2, '114': 0.8, '258': 0.2, '167': 0.8, '370': 0.8, '24': 0.0, '263': 0.5, '124': 0.6, '337': 1.0, '361': 0.6, '41': 0.8, '240': 0.0, '238': 0.4, '21': 1.0, '10': 1.0, '174': 0.0, '137': 0.8, '115': 0.5, '186': 0.4, '346': 0.4, '312': 0.8, '45': 0.2, '279': 0.4, '212': 0.4, '314': 0.8, '140': 1.0, '251': 0.4, '3': 0.6, '83': 0.4, '14': 0.2, '101': 0.6, '145': 0.5, '134': 0.4, '181': 0.4, '271': 0.2, '152': 0.6, '18': 0.4, '224': 0, '357': 0.6, '97': 0.6666666666666666, '202': 0.2, '142': 0.6, '311': 0.0, '150': 0.3333333333333333, '220': 1.0, '38': 0.2, '360': 1.0, '194': 0.0, '265': 0.0, '248': 0.2, '126': 1.0, '235': 0.4, '85': 0.6, '52': 1.0, '267': 0.8, '158': 0.2, '1': 0.8, '244': 0.6, '184': 0.4, '144': 0.6, '283': 0.8, '313': 0.0, '86': 0.5, '320': 0.75, '246': 0.2, '16': 0.75, '349': 1.0, '282': 0.75, '317': 0.6, '266': 0.8, '294': 0.4, '13': 0.5, '196': 0.2, '154': 0.8, '9': 0.3333333333333333, '293': 0.5, '195': 0.4, '100': 0.75, '109': 0.4, '234': 0.6666666666666666, '269': 0.4, '316': 0.2, '206': 0.2, '60': 0.8, '325': 0.5, '198': 0.6666666666666666, '305': 1.0, '127': 0.6, '302': 1.0, '343': 0.4, '227': 0.3333333333333333, '65': 1.0, '34': 0.8, '354': 0.3333333333333333, '201': 0.75, '168': 1.0, '105': 0.6666666666666666, '128': 0.8, '330': 0.4, '277': 0.4, '103': 0.4, '189': 0.8, '129': 0.2, '328': 0, '292': 0.4, '149': 0.3333333333333333, '243': 0.0, '299': 0.2, '254': 0.4, '157': 0.2, '68': 0.3333333333333333, '175': 0.0, '280': 0.6, '57': 0, '79': 1.0, '253': 0.8, '19': 0.5, '50': 1.0, '340': 0.8, '229': 0.6, '75': 1.0, '169': 0.6, '290': 1.0, '153': 0.8, '310': 0.0, '334': 0.4, '315': 1.0, '44': 1.0, '7': 0.4, '121': 0.8, '67': 0.6, '239': 0.4, '273': 0.4, '231': 0.0, '262': 1.0, '76': 1.0, '51': 0.6, '170': 0.8, '148': 0.5, '139': 0, '286': 0.75, '112': 0.6, '335': 1.0, '284': 0.4, '182': 0.4, '48': 0.8, '355': 0, '183': 0.75, '29': 1.0, '77': 0.2, '339': 0.8, '264': 0.4, '270': 0.8, '230': 0.6, '338': 0.6, '368': 0.2, '35': 1.0, '172': 0.0, '173': 1.0, '256': 0.8, '327': 0.5, '304': 0.6, '90': 0.0, '118': 1.0, '185': 0.25, '84': 0.5, '62': 1.0, '64': 1.0, '20': 0.0, '23': 0.6, '5': 0.8, '213': 0.8, '6': 0, '369': 0.0, '70': 0.2, '331': 0.4, '199': 0.5, '303': 0.6, '326': 0, '55': 0.0, '250': 1.0, '352': 0, '223': 1.0, '187': 0.0, '289': 0.0, '26': 0.4, '178': 0.75, '113': 0.0, '233': 0.4, '323': 0.4, '228': 0.0, '351': 0.6666666666666666, '291': 0.0, '74': 0.8, '210': 0.3333333333333333, '71': 0.75, '106': 0.5, '120': 0.0, '358': 0.0, '197': 0.5, '211': 1.0, '219': 0.3333333333333333, '163': 0.8, '342': 0.75, '180': 0.25, '161': 0.0, '93': 0.4, '249': 0.3333333333333333, '365': 0.5, '92': 0.0, '307': 0, '151': 0.0, '125': 1.0, '25': 0.2, '122': 0.8, '123': 0.0, '59': 0.6666666666666666, '102': 0.6, '225': 0.0, '222': 0.75, '162': 0.6, '200': 0.3333333333333333, '110': 0.6, '8': 1.0, '117': 1.0, '329': 0.6, '47': 0.25, '364': 0.3333333333333333, '252': 0.8, '301': 0.6, '321': 1.0, '276': 1.0, '37': 0.3333333333333333, '203': 0.5, '2': 0.5, '322': 1.0, '66': 1.0, '138': 0.8, '255': 1.0, '324': 0.2, '49': 0.6, '217': 0.0, '193': 0, '39': 1.0, '111': 0.4, '107': 0.5, '268': 1.0, '207': 0.5, '155': 0.4, '171': 0, '272': 0.4, '281': 1.0, '78': 1.0, '274': 0.3333333333333333, '300': 0, '363': 0.25, '259': 0.3333333333333333, '275': 1.0, '28': 1.0, '143': 0.5, '166': 0, '191': 1.0, '333': 1.0, '12': 1.0, '288': 0.0, '350': 0.2, '63': 1.0, '242': 0.5, '214': 0.5, '345': 0.5, '159': 1.0, '257': 0, '4': 1.0, '94': 0.3333333333333333, '80': 0, '81': 1.0, '221': 0.0, '308': 0.2, '261': 0.0, '260': 0.4, '95': 0.5, '146': 0.0, '297': 0, '15': 0.0, '353': 1.0, '36': 0.0, '177': 0, '87': 0.5, '179': 0.0, '216': 0, '341': 0.5, '226': 0, '356': 1.0, '17': 1.0, '287': 0, '362': 0.0}\n",
            "Algorithm  RMSE       MAE        HR         cHR        ARHR       Coverage   Diversity  Novelty   \n",
            "AutoRec    2.0064     1.6419     0.0054     0.0054     0.0032     1.0000     0.3066     1368.1203 \n",
            "Random     1.4573     1.1647     0.0135     0.0135     0.0076     1.0000     0.0511     864.3595  \n",
            "\n",
            "Legend:\n",
            "\n",
            "RMSE:      Root Mean Squared Error. Lower values mean better accuracy.\n",
            "MAE:       Mean Absolute Error. Lower values mean better accuracy.\n",
            "Map:       Mean Average Precision.\n",
            "HR:        Hit Rate; how often we are able to recommend a left-out rating. Higher is better.\n",
            "cHR:       Cumulative Hit Rate; hit rate, confined to ratings above a certain threshold. Higher is better.\n",
            "ARHR:      Average Reciprocal Hit Rank - Hit rate that takes the ranking into account. Higher is better.\n",
            "Coverage:  Ratio of users for whom recommendations above a certain threshold exist. Higher is better.\n",
            "Diversity: 1-S, where S is the average similarity score between every possible pair of recommendations\n",
            "           for a given user. Higher means more diverse.\n",
            "Novelty:   Average popularity rank of recommended items. Higher means more novel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluator.SampleTopNRecs(ml)\n"
      ],
      "metadata": {
        "id": "G5GdU0_pfSDK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f63609a-36ab-4f10-e73a-5b915377322a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using recommender  AutoRec\n",
            "\n",
            "Building recommendation model...\n",
            "Trained epoch  0\n",
            "Trained epoch  1\n",
            "Trained epoch  2\n",
            "Trained epoch  3\n",
            "Trained epoch  4\n",
            "Trained epoch  5\n",
            "Trained epoch  6\n",
            "Trained epoch  7\n",
            "Trained epoch  8\n",
            "Trained epoch  9\n",
            "Trained epoch  10\n",
            "Trained epoch  11\n",
            "Trained epoch  12\n",
            "Trained epoch  13\n",
            "Trained epoch  14\n",
            "Trained epoch  15\n",
            "Trained epoch  16\n",
            "Trained epoch  17\n",
            "Trained epoch  18\n",
            "Trained epoch  19\n",
            "Trained epoch  20\n",
            "Trained epoch  21\n",
            "Trained epoch  22\n",
            "Trained epoch  23\n",
            "Trained epoch  24\n",
            "Trained epoch  25\n",
            "Trained epoch  26\n",
            "Trained epoch  27\n",
            "Trained epoch  28\n",
            "Trained epoch  29\n",
            "Trained epoch  30\n",
            "Trained epoch  31\n",
            "Trained epoch  32\n",
            "Trained epoch  33\n",
            "Trained epoch  34\n",
            "Trained epoch  35\n",
            "Trained epoch  36\n",
            "Trained epoch  37\n",
            "Trained epoch  38\n",
            "Trained epoch  39\n",
            "Trained epoch  40\n",
            "Trained epoch  41\n",
            "Trained epoch  42\n",
            "Trained epoch  43\n",
            "Trained epoch  44\n",
            "Trained epoch  45\n",
            "Trained epoch  46\n",
            "Trained epoch  47\n",
            "Trained epoch  48\n",
            "Trained epoch  49\n",
            "Trained epoch  50\n",
            "Trained epoch  51\n",
            "Trained epoch  52\n",
            "Trained epoch  53\n",
            "Trained epoch  54\n",
            "Trained epoch  55\n",
            "Trained epoch  56\n",
            "Trained epoch  57\n",
            "Trained epoch  58\n",
            "Trained epoch  59\n",
            "Trained epoch  60\n",
            "Trained epoch  61\n",
            "Trained epoch  62\n",
            "Trained epoch  63\n",
            "Trained epoch  64\n",
            "Trained epoch  65\n",
            "Trained epoch  66\n",
            "Trained epoch  67\n",
            "Trained epoch  68\n",
            "Trained epoch  69\n",
            "Trained epoch  70\n",
            "Trained epoch  71\n",
            "Trained epoch  72\n",
            "Trained epoch  73\n",
            "Trained epoch  74\n",
            "Trained epoch  75\n",
            "Trained epoch  76\n",
            "Trained epoch  77\n",
            "Trained epoch  78\n",
            "Trained epoch  79\n",
            "Trained epoch  80\n",
            "Trained epoch  81\n",
            "Trained epoch  82\n",
            "Trained epoch  83\n",
            "Trained epoch  84\n",
            "Trained epoch  85\n",
            "Trained epoch  86\n",
            "Trained epoch  87\n",
            "Trained epoch  88\n",
            "Trained epoch  89\n",
            "Trained epoch  90\n",
            "Trained epoch  91\n",
            "Trained epoch  92\n",
            "Trained epoch  93\n",
            "Trained epoch  94\n",
            "Trained epoch  95\n",
            "Trained epoch  96\n",
            "Trained epoch  97\n",
            "Trained epoch  98\n",
            "Trained epoch  99\n",
            "Processing user  0\n",
            "Processing user  50\n",
            "Processing user  100\n",
            "Processing user  150\n",
            "Processing user  200\n",
            "Processing user  250\n",
            "Processing user  300\n",
            "Processing user  350\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "Little Big Man (1970) 5\n",
            "Repo Man (1984) 5\n",
            "American Beauty (1999) 5\n",
            "Pinocchio (1940) 5\n",
            "Best in Show (2000) 5\n",
            "Family Man, The (2000) 5\n",
            "Tootsie (1982) 5\n",
            "Species II (1998) 5\n",
            "Predator 2 (1990) 5\n",
            "100 Girls (2000) 5\n",
            "\n",
            "Using recommender  Random\n",
            "\n",
            "Building recommendation model...\n",
            "Computing recommendations...\n",
            "\n",
            "We recommend:\n",
            "Rumble in the Bronx (Hont faan kui) (1995) 5\n",
            "Rosencrantz and Guildenstern Are Dead (1990) 5\n",
            "Indiana Jones and the Last Crusade (1989) 5\n",
            "Navigator: A Mediaeval Odyssey, The (1988) 5\n",
            "Austin Powers: The Spy Who Shagged Me (1999) 5\n",
            "Ghostbusters (a.k.a. Ghost Busters) (1984) 5\n",
            "Highlander: Endgame (Highlander IV) (2000) 5\n",
            "Harry Potter and the Sorcerer's Stone (a.k.a. Harry Potter and the Philosopher's Stone) (2001) 5\n",
            "Ring, The (2002) 5\n",
            "Chitty Chitty Bang Bang (1968) 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ml)"
      ],
      "metadata": {
        "id": "vlq9Q9u4fa5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c72e11e6-8ccf-45cf-aa68-c652d23a77eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.MovieLens object at 0x7f1db2b218d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ml.getMovieName(10))"
      ],
      "metadata": {
        "id": "gTB4ia76hZS6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0415b9d3-154d-4556-d51c-d66f044f422d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GoldenEye (1995)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "V1_MpS6bhtA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 9408.407449,
      "end_time": "2020-11-18T21:51:31.734212",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-11-18T19:14:43.326763",
      "version": "2.1.0"
    },
    "colab": {
      "name": "recommender_system_using_auto_encoders_working_hit-ratio.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}